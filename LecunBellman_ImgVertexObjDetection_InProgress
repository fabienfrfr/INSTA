#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Oct  5 21:10:33 2020
@author: fabien
"""
import numpy as np, pylab as plt, tensorflow as tf
from tensorflow.keras import models, layers
import cv2, skvideo.io, scipy as sp

################################### PARAMETER
EPOCH = 10000
# Net
BATCH_SIZE, GAMMA = 128, tf.constant(0.9)
NBR_IMG, NBR_ACTION = 1, 5
NBR_TF_ACTION = tf.constant(NBR_ACTION)
# Polygon
NBR_VERTEX = 4
# Image
SIZE = 32
# Dilemna
EPSILON_START, EPSILON_END = 1., 0.1
DECAY_RATE = 0.001
COEFF, POLITICS = (2./3, 1./3), ['exemple','exploration','exploitation']
# Game rule
CONST_ERR, CYCLE_MAX = 4, SIZE**2
BEST_SCORE = (np.pi)*SIZE

################################### MODEL  
# Network Architechture
def Net():
    # image
    entree = layers.Input(shape=(SIZE,SIZE, 1), dtype='float32')
    # 4 couche de convolution
    result = layers.Conv2D(8, 3, activation='relu', padding='same', strides=2)(entree/8)
    result = layers.Conv2D(16, 3, activation='relu', padding='same', strides=2)(result)
    result = layers.BatchNormalization()(result)
    result = layers.Conv2D(32, 3, activation='relu', padding='same', strides=2)(result)
    result = layers.Conv2D(64, 3, activation='relu', padding='same', strides=2)(result)
    result = layers.BatchNormalization()(result)
    # 1 couche dense
    result = layers.Flatten()(result)
    result = layers.Dense(512, activation='relu')(result)
    # action
    sortie = layers.Dense(NBR_ACTION)(result)
    return models.Model(inputs=entree, outputs=sortie)

################################### ENVIRONMENT
def Update_pos(position, action) :
    if action == 4 :
        new_position = position
    else :
        new_position = position
        # PosXY update
        idx = action % 2
        if action > 1 :
            new_position[idx] = np.mod(position[idx]-1, SIZE)
        else :
            new_position[idx] = np.mod(position[idx]+1, SIZE)
    return new_position
    
def Update_img(image, empty, end_position) :
    new_image = image.copy()
    # Empty image actualisation
    if len(empty) > 0 : 
        for h in empty : new_image[tuple(h)] = 0
    # Localisation in image
    new_image[tuple(end_position)] = 7
    return [new_image]

class Env():
    def __init__(self):
        self.EUCLID = [None,None] # position, polygon
        self.IMG = [None,None,[]] # initial, observation, hole_list
        self.VAR_OUT = [None,False] # Reward, Done
    def reset(self):
        self.__init__()
        # Initialize position
        self.EUCLID[0] = np.random.randint(0,SIZE, (2))
        # Polygone generator
        p = np.random.randint(0, SIZE, (NBR_VERTEX,2))
        s = np.arctan2((p-p.mean(axis=0))[:,0], (p-p.mean(axis=0))[:,1])
        self.EUCLID[1] = p[np.argsort(s),:]
        # Image initialisation (BG = 1)
        img, c = np.ones((SIZE,SIZE), np.uint8), np.random.randint(2, 7)
        self.IMG[0] = np.swapaxes(cv2.drawContours(img, [self.EUCLID[1]], -1, c, -1), 0,1)
        # Update image
        self.IMG[1] = Update_img(self.IMG[0], [], self.EUCLID[0])
        return self.IMG[1]
    def step(self, action):
        # Games rules
        if action != 4 :
            self.VAR_OUT[0] = 0
            # Update position
            self.EUCLID[0] = Update_pos(self.EUCLID[0], action)
        else :
            self.IMG[2] += [self.EUCLID[0].copy()]
            dst = np.linalg.norm(self.EUCLID[0] - self.EUCLID[1], axis=1)
            if dst.min() > np.sqrt(2) : 
                self.VAR_OUT[0] = -10
            else :
                self.VAR_OUT[0] = +10
                # Update vertex solution
                idx = int(np.where(dst == dst.min())[0][0])
                self.EUCLID[1] = np.delete(self.EUCLID[1], idx,0)
        # Update image
        self.IMG[1] = Update_img(self.IMG[0], self.IMG[2], self.EUCLID[0])
        # Condition stop
        if len(self.IMG[2]) > NBR_VERTEX*CONST_ERR :
            self.VAR_OUT[0] = -50
            self.VAR_OUT[1] = True
        elif len(self.EUCLID[1]) == 0 :
            self.VAR_OUT[0] = +50
            self.VAR_OUT[1] = True
        return self.IMG[1], self.VAR_OUT[0], self.VAR_OUT[1]
        
################################### LEARNING
def Sol(dilemna, observation):
    if dilemna == 'exemple' :
        # Euclidean distance between position and polygon vertex
        d = np.linalg.norm(env.EUCLID[0] - env.EUCLID[1], axis=1)
        if d.min() == 0 : 
            next_action = 4
        else :
            # Calculate variation following (x,y) axis
            VAR = env.EUCLID[0] - env.EUCLID[1][np.where(d == d.min())[0]][0]
            # Choosing axis mvt
            if abs(VAR).min() != 0 :
                idx = int(np.where(abs(VAR)==abs(VAR).min())[0][0])
            else :
                idx = int(np.where(abs(VAR)!=abs(VAR).min())[0][0])
            # Direction in axis
            sign = -np.sign(VAR[idx])
            if (idx == 0) and (sign == 1) : next_action = 0
            elif (idx == 1) and (sign == 1) : next_action = 1
            elif (idx == 0) and (sign == -1) : next_action = 2
            elif (idx == 1) and (sign == -1) : next_action = 3
    elif dilemna == 'exploration' :
        # Non linear randomized number
        next_action = np.random.choice(5, 1, p=4*[0.24]+[0.04])[0]
    else :
        # Passing image in Network
        output = model(np.expand_dims(observation, axis=0))
        # Max values classification
        next_action = int(tf.argmax(output[0], axis=-1))
    return next_action

# Bellman equation (only tf format recommended)
@tf.function
def train(observation, action, next_observation, reward, done):
    # Q' (=after action)
    next_Q_values = model(next_observation)
    # GPU Q_best 
    best_next_actions = tf.math.argmax(next_Q_values, axis=1)
    next_mask = tf.one_hot(best_next_actions, NBR_TF_ACTION)
    next_best_Q_values = tf.reduce_sum(next_Q_values*next_mask, axis=1)
    # Q_target
    target_Q_values = reward+(1-done)*GAMMA*next_best_Q_values
    target_Q_values = tf.reshape(target_Q_values, (-1, 1))
    # Correction
    mask = tf.one_hot(action, NBR_TF_ACTION)
    with tf.GradientTape() as tape:
        all_Q_values = model(observation)
        Q_values = tf.reduce_sum(all_Q_values*mask, axis=1, keepdims=True)
        loss = tf.reduce_mean(tf.math.square(target_Q_values - Q_values))
    gradients = tape.gradient(loss, model.trainable_variables)
    # Retropropagate gradiant descent
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    criterion(loss)
    criterion.reset_states()

################################### ALGORITHMIC'S PART
def Fit(model):
    global env, M
    # Initialize politics
    epsilon = EPSILON_START
    # Initialize control-environment
    env = Env()
    # Train loop
    for e in np.arange(EPOCH):
        # Sample initialisation (memory : 'old_state', 'action', 'new_state', 'reward', 'terminal')
        M = [[],[],[],[],[]]
        # Reset environment
        observation = env.reset()
        # Action loop
        cycle_step = 0
        while cycle_step < CYCLE_MAX :
            M[0].append(observation)
            # Exemple/Exploration/Exploitation Dilemme
            dilemna = np.random.choice(POLITICS, 1, p=[COEFF[0]*epsilon, COEFF[1]*epsilon, 1-epsilon])[0]
            # Calculate solution
            action = Sol(dilemna, observation[0])
            # Update sample
            observation, reward, done = env.step(action)
            # Update sample
            M[1].append(action), M[2].append(observation), M[3].append(reward), M[4].append(done)
            # Ending loop :
            if done : break
            else : 
                cycle_step += 1
        # Display result
        print("EPOCH:", e, "Epsilon", epsilon, "Cycle", cycle_step)
        # Prepare sample batch 
        old_state_ = tf.convert_to_tensor(np.array(M[0]).squeeze(), dtype=tf.float32)
        action_ = tf.convert_to_tensor(np.array(M[1]), dtype=tf.int32)
        new_state_ = tf.convert_to_tensor(np.array(M[2]).squeeze(), dtype=tf.float32)
        reward_ = tf.convert_to_tensor(np.array(M[3]), dtype=tf.float32)
        terminal_ = tf.convert_to_tensor(np.array(M[4]), dtype=tf.float32)
        # Reinforcement training
        train(old_state_, action_, new_state_, reward_, terminal_)
        # Save the last observation (optional)
        if e % 10 == 0 :
            video = (255*np.array(M[2], dtype=np.float32).squeeze()/8).astype(np.int)
            skvideo.io.vwrite("OUT/outputvideo_"+str(e)+".mp4", video)
        # Update politics
        if epsilon > EPSILON_END : epsilon -= DECAY_RATE
        # Ending
        if (epsilon < EPSILON_END) and (cycle_step < BEST_SCORE) : break
    # Update model
    model.save('QmodelTrace')

def Predict(model):
    # Importing a image
    img_rgb = cv2.imread(DIR)
    # Normalise image
    img_ = img.mean()
    # Normalise&Contouring image by 6 values
    img_ = [1,6] # 0 and 7 for action/empty
    # Adding position
    img[randomTuple] = 0
    # Action loop
    while True :
        action = Sol('exploitation')
        # Update position
        position = Update_pos(posList, action)
        # Update image
        img, empty = Update_img(img_,empty,position)
    # List of polygons
    return posList

################################### MAIN
def main():
    global model, optimizer, criterion
    # Initialize neural network parameters and optimizer
    model = Net()
    optimizer = tf.keras.optimizers.Adam(learning_rate=1E-4)
    criterion = tf.keras.metrics.Mean()
    # Adjust model with polygon in image
    try : 
        model = models.load_model('QmodelTrace')
    except : Fit(model)
    # Predict polygons in image
    polygons = Predict(model)
    # Construct vectorial imag
    plt.fill(posList, color = img_rgb[:].mean())
    # Save in SVG format
    plt.savefig(NAME + '.svg')

if __name__ == "__main__":
	main()
